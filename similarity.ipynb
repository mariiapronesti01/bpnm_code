{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMILARITY MEASURE\n",
    "It will be a weighted average of similarity scores which takes into account topological features of the graph and content of nodes labels.\n",
    "\n",
    "**Similairty scores** compare any node of G1 to any node of G2 and assign a score based on:\n",
    "- type of nodes (need to define some rules)\n",
    "- content of the labels (contextul similarity using SBERT model)\n",
    "- distance from closest start/end nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\SNA_env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random \n",
    "\n",
    "from similarity_utils import *\n",
    "from parser_with_lane import get_edge_df_from_bpmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\SNA_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "#model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')#all-mpnet-base-v2')\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_similarity(file1, file2, model, verbose=False):\n",
    "    edge_df1 = get_edge_df_from_bpmn(file1)\n",
    "    edge_df2 = get_edge_df_from_bpmn(file2)\n",
    "\n",
    "    G1 = obtain_graph(edge_df1)\n",
    "    G2 = obtain_graph(edge_df2)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Graph 1 has:\", G1.number_of_nodes(), \"nodes and\", G1.number_of_edges(), \"edges\")\n",
    "        print(f\"Graph 2 has:\", G2.number_of_nodes(), \"nodes and\", G2.number_of_edges(), \"edges\")\n",
    "\n",
    "    _, _, _, _, similarity_matrix = get_similarity_matrix(G1, G2, model)\n",
    "    return get_similarity_measure(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of 12 folders from the base path\n",
    "def load_files_from_folders(base_path, subset_size=4):\n",
    "    folders = {}\n",
    "    # List all folders in the base directory\n",
    "    all_folders = [folder_name for folder_name in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, folder_name))]\n",
    "    \n",
    "    # Select a random subset of 12 folders\n",
    "    selected_folders = random.sample(all_folders, min(subset_size, len(all_folders)))\n",
    "\n",
    "    # Load files from the selected folders\n",
    "    for folder_name in selected_folders:\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        folders[folder_name] = files\n",
    "\n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = load_files_from_folders(\"dati\\\\bpmn\", subset_size=4)\n",
    "all_files = [file for folder in folders.values() for file in folder]\n",
    "all_files = [file for file in all_files if file.endswith('.xml')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dati\\\\bpmn\\\\M_j01\\\\0.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\M_j01\\\\1.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\M_j01\\\\2.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\M_j01\\\\3.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\M_j01\\\\4.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\M_j01\\\\5.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\M_j01\\\\6.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\M_j01\\\\7.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\M_j01\\\\8.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\M_j01\\\\9.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j04\\\\0.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j04\\\\1.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j04\\\\2.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j04\\\\3.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j04\\\\4.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j04\\\\5.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j04\\\\6.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j04\\\\7.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\0.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\1.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\10.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\11.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\2.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\3.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\4.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\5.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\6.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\7.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\8.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_g01\\\\9.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\0.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\1.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\10.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\2.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\3.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\4.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\5.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\6.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\7.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\8.bpmn2.xml',\n",
       " 'dati\\\\bpmn\\\\R_j01\\\\9.bpmn2.xml']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_similarity_matrix(all_files, model, n_jobs=-1):\n",
    "#     n = len(all_files)\n",
    "#     similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "#     Parallel computation of the upper triangular matrix\n",
    "#     def compute_similarity(i, j):\n",
    "#         similarity = check_file_similarity(all_files[i], all_files[j], model)\n",
    "#         similarity_matrix[i, j] = similarity\n",
    "#         similarity_matrix[j, i] = similarity  # Exploit symmetry\n",
    "\n",
    "#     Parallel(n_jobs=n_jobs)(\n",
    "#         delayed(compute_similarity)(i, j) for i in range(n) for j in range(i, n)\n",
    "#     )\n",
    "#     return similarity_matrix\n",
    "\n",
    "def compute_similarity_matrix(all_files, model):\n",
    "    n = len(all_files)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            # Compute similarity once for (i, j) and reuse it for (j, i)\n",
    "            print(f\"Computing similarity between {all_files[i]} and {all_files[j]}\")\n",
    "            similarity = check_file_similarity(all_files[i], all_files[j], model)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "            similarity_matrix[j, i] = similarity  # Exploit symmetry\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\0.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\1.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\Desktop\\tirocinio\\bpnm_code\\similarity_utils.py:81: RuntimeWarning: invalid value encountered in divide\n",
      "  shortest_path_distance = 1 - (diff / max_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\2.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 2 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\3.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\4.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\5.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\6.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\7.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\8.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\M_j01\\9.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 3 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\R_j04\\0.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\R_j04\\1.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\R_j04\\2.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\R_j04\\3.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\R_j04\\4.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\R_j04\\5.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 3 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\R_j04\\6.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\R_j04\\7.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 1 processes in the BPMN file\n",
      "Computing similarity between dati\\bpmn\\M_j01\\0.bpmn2.xml and dati\\bpmn\\R_g01\\0.bpmn2.xml\n",
      "Detected 1 processes in the BPMN file\n",
      "Detected 3 processes in the BPMN file\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_similarity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 24\u001b[0m, in \u001b[0;36mcompute_similarity_matrix\u001b[1;34m(all_files, model)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i, n):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Compute similarity once for (i, j) and reuse it for (j, i)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing similarity between \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_files[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_files[j]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_file_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     similarity_matrix[i, j] \u001b[38;5;241m=\u001b[39m similarity\n\u001b[0;32m     26\u001b[0m     similarity_matrix[j, i] \u001b[38;5;241m=\u001b[39m similarity  \u001b[38;5;66;03m# Exploit symmetry\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mcheck_file_similarity\u001b[1;34m(file1, file2, model, verbose)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph 1 has:\u001b[39m\u001b[38;5;124m\"\u001b[39m, G1\u001b[38;5;241m.\u001b[39mnumber_of_nodes(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes and\u001b[39m\u001b[38;5;124m\"\u001b[39m, G1\u001b[38;5;241m.\u001b[39mnumber_of_edges(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph 2 has:\u001b[39m\u001b[38;5;124m\"\u001b[39m, G2\u001b[38;5;241m.\u001b[39mnumber_of_nodes(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes and\u001b[39m\u001b[38;5;124m\"\u001b[39m, G2\u001b[38;5;241m.\u001b[39mnumber_of_edges(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m _, _, _, _, similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mget_similarity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_similarity_measure(similarity_matrix)\n",
      "File \u001b[1;32mc:\\Users\\maria\\Desktop\\tirocinio\\bpnm_code\\similarity_utils.py:152\u001b[0m, in \u001b[0;36mget_similarity_matrix\u001b[1;34m(G1, G2, model)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Precompute embeddings for all node labels in G1 and G2\u001b[39;00m\n\u001b[0;32m    151\u001b[0m G1_embeddings \u001b[38;5;241m=\u001b[39m {node: model\u001b[38;5;241m.\u001b[39mencode(G1\u001b[38;5;241m.\u001b[39mnodes[node][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G1\u001b[38;5;241m.\u001b[39mnodes}\n\u001b[1;32m--> 152\u001b[0m G2_embeddings \u001b[38;5;241m=\u001b[39m {node: model\u001b[38;5;241m.\u001b[39mencode(G2\u001b[38;5;241m.\u001b[39mnodes[node][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G2\u001b[38;5;241m.\u001b[39mnodes}\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Precompute degrees and types for faster access during loops\u001b[39;00m\n\u001b[0;32m    155\u001b[0m G1_degrees \u001b[38;5;241m=\u001b[39m {node: (G1\u001b[38;5;241m.\u001b[39min_degree(node), G1\u001b[38;5;241m.\u001b[39mout_degree(node)) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G1\u001b[38;5;241m.\u001b[39mnodes}\n",
      "File \u001b[1;32mc:\\Users\\maria\\Desktop\\tirocinio\\bpnm_code\\similarity_utils.py:152\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Precompute embeddings for all node labels in G1 and G2\u001b[39;00m\n\u001b[0;32m    151\u001b[0m G1_embeddings \u001b[38;5;241m=\u001b[39m {node: model\u001b[38;5;241m.\u001b[39mencode(G1\u001b[38;5;241m.\u001b[39mnodes[node][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G1\u001b[38;5;241m.\u001b[39mnodes}\n\u001b[1;32m--> 152\u001b[0m G2_embeddings \u001b[38;5;241m=\u001b[39m {node: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G2\u001b[38;5;241m.\u001b[39mnodes}\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Precompute degrees and types for faster access during loops\u001b[39;00m\n\u001b[0;32m    155\u001b[0m G1_degrees \u001b[38;5;241m=\u001b[39m {node: (G1\u001b[38;5;241m.\u001b[39min_degree(node), G1\u001b[38;5;241m.\u001b[39mout_degree(node)) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G1\u001b[38;5;241m.\u001b[39mnodes}\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\envs\\SNA_env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:569\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    568\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m--> 569\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    571\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\envs\\SNA_env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1024\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;124;03m            \"attention_mask\", and \"token_type_ids\".\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\envs\\SNA_env\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:154\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts, padding)\u001b[0m\n\u001b[0;32m    152\u001b[0m batch1, batch2 \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m--> 154\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    155\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    156\u001b[0m to_tokenize \u001b[38;5;241m=\u001b[39m [batch1, batch2]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "similarity_matrix = compute_similarity_matrix(all_files, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the similarity matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(similarity_matrix, annot=True, xticklabels=all_files, yticklabels=all_files, cmap='YlGn')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
